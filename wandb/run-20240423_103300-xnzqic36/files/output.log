  0%|                                                                                          | 0/6680 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/s/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")






































































































  5%|███▉                                                                          | 334/6680 [03:26<1:05:54,  1.60it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.37372085452079773, 'eval_accuracy': 0.863568215892054, 'eval_precision': 0.810561061980893, 'eval_recall': 0.7788704260121756, 'eval_f1': 0.7930673927271974, 'eval_runtime': 2.4767, 'eval_samples_per_second': 538.624, 'eval_steps_per_second': 2.423, 'epoch': 1.0}



















































  7%|█████▊                                                                        | 500/6680 [05:13<1:03:15,  1.63it/s]




















































 10%|███████▊                                                                      | 668/6680 [06:59<1:03:43,  1.57it/s]

  0%|                                                                                             | 0/6 [00:00<?, ?it/s]
 10%|███████▊                                                                      | 668/6680 [07:02<1:03:43,  1.57it/s]/home/s/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









































































































 15%|███████████▌                                                                 | 1000/6680 [10:34<1:02:42,  1.51it/s]
 15%|███████████▌                                                                 | 1002/6680 [10:36<1:01:07,  1.55it/s]

 33%|████████████████████████████▎                                                        | 2/6 [00:00<00:00,  4.41it/s]
 15%|███████████▌                                                                 | 1002/6680 [10:38<1:01:07,  1.55it/s]/home/s/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








































































































 20%|███████████████▊                                                               | 1336/6680 [14:09<54:20,  1.64it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 20%|███████████████▍                                                             | 1337/6680 [14:13<2:14:59,  1.52s/it]



















































 22%|█████████████████▊                                                             | 1501/6680 [15:55<52:20,  1.65it/s]




















































 25%|███████████████████▊                                                           | 1670/6680 [17:40<52:50,  1.58it/s]

 33%|████████████████████████████▎                                                        | 2/6 [00:00<00:00,  4.61it/s]
 25%|███████████████████▊                                                           | 1670/6680 [17:42<52:50,  1.58it/s]/home/s/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")




































































































 30%|███████████████████████▌                                                       | 1997/6680 [21:05<47:31,  1.64it/s]


 30%|███████████████████████▋                                                       | 2004/6680 [21:09<48:00,  1.62it/s]
 83%|██████████████████████████████████████████████████████████████████████▊              | 5/6 [00:01<00:00,  2.63it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")




































































































 35%|███████████████████████████▋                                                   | 2338/6680 [24:38<44:29,  1.63it/s]
 33%|████████████████████████████▎                                                        | 2/6 [00:00<00:00,  4.72it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

















































 37%|█████████████████████████████▌                                                 | 2499/6680 [26:21<42:59,  1.62it/s]






































 40%|███████████████████████████████▌                                               | 2672/6680 [27:38<44:48,  1.49it/s]
 50%|██████████████████████████████████████████▌                                          | 3/6 [00:00<00:00,  3.26it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")




































































































 45%|███████████████████████████████████▍                                           | 2998/6680 [31:04<38:07,  1.61it/s]


 45%|███████████████████████████████████▌                                           | 3006/6680 [31:09<37:54,  1.62it/s]
 33%|████████████████████████████▎                                                        | 2/6 [00:00<00:00,  4.77it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







































































































 50%|███████████████████████████████████████▌                                       | 3340/6680 [34:40<34:16,  1.62it/s]
 67%|████████████████████████████████████████████████████████▋                            | 4/6 [00:01<00:00,  2.71it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")














































 52%|█████████████████████████████████████████▎                                     | 3498/6680 [36:16<32:36,  1.63it/s]


























































 55%|███████████████████████████████████████████▍                                   | 3674/6680 [38:12<30:53,  1.62it/s]
 67%|████████████████████████████████████████████████████████▋                            | 4/6 [00:01<00:00,  2.76it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")






































































































 60%|███████████████████████████████████████████████▎                               | 4000/6680 [41:41<27:46,  1.61it/s]


 60%|███████████████████████████████████████████████▍                               | 4008/6680 [41:46<27:55,  1.59it/s]
 33%|████████████████████████████▎                                                        | 2/6 [00:00<00:00,  4.69it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")


































































































 65%|███████████████████████████████████████████████████▎                           | 4342/6680 [45:09<11:10,  3.49it/s]
 33%|████████████████████████████▎                                                        | 2/6 [00:00<00:00,  8.26it/s]

 65%|███████████████████████████████████████████████████▎                           | 4342/6680 [45:10<11:10,  3.49it/s]/home/s/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")



















































 67%|█████████████████████████████████████████████████████▏                         | 4499/6680 [46:55<22:15,  1.63it/s]























































 70%|███████████████████████████████████████████████████████▎                       | 4676/6680 [48:46<20:52,  1.60it/s]

  0%|                                                                                             | 0/6 [00:00<?, ?it/s]
 70%|███████████████████████████████████████████████████████▎                       | 4676/6680 [48:49<20:52,  1.60it/s]/home/s/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")




































































































 75%|███████████████████████████████████████████████████████████                    | 4999/6680 [52:11<17:32,  1.60it/s]



 75%|███████████████████████████████████████████████████████████▎                   | 5010/6680 [52:18<17:23,  1.60it/s]
 33%|████████████████████████████▎                                                        | 2/6 [00:00<00:00,  4.65it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







































































































 80%|███████████████████████████████████████████████████████████████▏               | 5344/6680 [55:49<13:44,  1.62it/s]
 67%|████████████████████████████████████████████████████████▋                            | 4/6 [00:01<00:00,  2.74it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
















































 82%|█████████████████████████████████████████████████████████████████              | 5498/6680 [57:29<12:12,  1.61it/s]

























































 85%|███████████████████████████████████████████████████████████████████▏           | 5678/6680 [59:24<10:43,  1.56it/s]
 67%|████████████████████████████████████████████████████████▋                            | 4/6 [00:01<00:00,  2.76it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")




































































































 90%|█████████████████████████████████████████████████████████████████████▏       | 5998/6680 [1:02:48<06:56,  1.64it/s]




 90%|█████████████████████████████████████████████████████████████████████▎       | 6012/6680 [1:02:57<06:42,  1.66it/s]
 50%|██████████████████████████████████████████▌                                          | 3/6 [00:00<00:00,  3.22it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







































































































 95%|█████████████████████████████████████████████████████████████████████████▏   | 6346/6680 [1:06:29<03:29,  1.59it/s]
 50%|██████████████████████████████████████████▌                                          | 3/6 [00:00<00:00,  3.38it/s]
  warnings.warn(
/home/s/anaconda3/envs/luongtk/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")















































 97%|██████████████████████████████████████████████████████████████████████████▉  | 6497/6680 [1:08:08<01:52,  1.63it/s]

























































100%|█████████████████████████████████████████████████████████████████████████████| 6680/6680 [1:10:03<00:00,  1.61it/s]
 67%|████████████████████████████████████████████████████████▋                            | 4/6 [00:01<00:00,  2.89it/s]
{'eval_loss': 0.2570675313472748, 'eval_accuracy': 0.904047976011994, 'eval_precision': 0.8549129042181898, 'eval_recall': 0.8565021912991747, 'eval_f1': 0.8556488118921171, 'eval_runtime': 2.4349, 'eval_samples_per_second': 547.874, 'eval_steps_per_second': 2.464, 'epoch': 20.0}

{'train_runtime': 4209.5532, 'train_samples_per_second': 50.699, 'train_steps_per_second': 1.587, 'train_loss': 0.25758882111418036, 'epoch': 20.0}